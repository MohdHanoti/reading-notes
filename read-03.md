# Class read 03

## A beginner's guide to Big O Notation

Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm.

Big O notation will always assume the upper limit where the algorithm will perform the maximum number of iterations.

Logarithmic time complexity log(n): Represented in Big O notation as O(log n), when an algorithm has O(log n) running time, it means that as the input size grows, the number of operations grows very slowly.and the oposite is correct to.

in other words, Doubling the size of the input data set has little effect on its growth as after a single iteration of the algorithm the data set will be halved and therefore on a par with an input data set half the size. This makes algorithms like binary search extremely efficient when dealing with large data sets.

[read more](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation)

## Big O notation

Big O notation is one of the most fundamental tools for computer scientists to analyze the cost of an algorithm. It is a good practice for software engineers to understand in-depth as well. Big O notation describes the complexity of your code using algebraic terms.

it is the language we use for talking about how long an algorithm takes to run (time complexity) or how much memory is used by an algorithm (space complexity). it can express the best, worst, and average-case running time of an algorithm.

[Check this](https://www.codenewbie.org/basecs/8)
